{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Imports and Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Error to Handle Entries Incompatible for the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnableToCreateTableError(Exception):\n",
    "    def __init__(self, message = 'The number of entries in the scraped column does not equal the number of columns provided; thus, Pandas won\\'t be able to create this table.'):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Column Names for the Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column names from the website\n",
    "column_names = 'Player, Position, Age, Team, Games, GS, Minutes, '\n",
    "column_names += 'FG, FGA, FG%, 3P, 3PA, 3P%, 2P, 2PA, 2P%, FG%, FT, FTA, FT%, '\n",
    "column_names += 'ORB, DRB, TRB, AST, STL, BLK, TOV, PF, PT'\n",
    "columns = column_names.split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the Data for Two Different Relevant Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_write(nba_url, columns, timeline):\n",
    "    '''\n",
    "    Inputs:\n",
    "    nba_url - A string representing the URL of the NBA page you are trying to scrape\n",
    "    columns - A list of strings containing the names of the columns in the DataFrame\n",
    "              you're trying to create\n",
    "    timeline - A string with the part of the season in which you're scraping the data\n",
    "    \n",
    "    This function scrapes the page given by the URL and finds the table object. It then\n",
    "    creates a pandas DataFrame and saves it to a .csv file in the same directory.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    out_filename = 'nba_player_statistics_2019_2020_{}.csv'.format(timeline)\n",
    "    \n",
    "    #Get the html for the given url\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #Find tables in the html\n",
    "    table = soup.find('table')\n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    #Get the rows of the table\n",
    "    rows = table_body.find_all('tr')\n",
    "    \n",
    "    #Parse each row of data and convert entries to the correct data types\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        if len(cols) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                assert len(cols) == len(columns)\n",
    "            except:\n",
    "                raise UnableToCreateTableError()\n",
    "            for i in range(len(cols)):\n",
    "                if i not in [0,1,3]: \n",
    "                    if i in [2,4,5]:\n",
    "                        cols[i] = int(cols[i])\n",
    "                    else:\n",
    "                        if cols[i] == '':\n",
    "                            cols[i] = 0.0\n",
    "                        else:\n",
    "                            cols[i] = float(cols[i])\n",
    "            data.append(cols)\n",
    "            \n",
    "    nba = pd.DataFrame(data = data, columns = columns)\n",
    "    nba = nba.drop_duplicates(subset = 'Player', keep = 'last')\n",
    "    nba.to_csv(out_filename, index = False)\n",
    "    print('Finished ' + timeline)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reg_season\n",
      "Finished playoffs\n"
     ]
    }
   ],
   "source": [
    "season = 'https://www.basketball-reference.com/leagues/NBA_2020_per_game.html'\n",
    "playoffs = 'https://www.basketball-reference.com/playoffs/NBA_2020_per_game.html'\n",
    "\n",
    "urls = [season, playoffs]\n",
    "timelines = ['reg_season', 'playoffs']\n",
    "\n",
    "for url, time in zip(urls, timelines):\n",
    "    scrape_and_write(url, columns, time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
